?knn
?kknn
library(knn)
library("kknn", lib.loc="~/R/win-library/3.5")
?kknn
?knn
?train.kknn
tecator <- read_xlsx("tecator.xlsx")
library(readxl)
tecator <- read_xlsx("tecator.xlsx")
setwd("E:/Part-2/Machine Learning, 732A95/Machine Learning Labs/Lab 1 - Block 1")
tecator <- read_xlsx("tecator.xlsx")
fit <- lm(Fat~tecator[,2:101],data= tecator)
View(tecator)
?lm
data = read_excel("tecator.xlsx")
typeof(data)
typeof(tecator)
fit <- lm(Fat~(tecator[,2:101]),data= tecator)
dd <- tecator[,2:101]
fit <- lm(Fat~dd,data= tecator)
dd <- tecator[,2:101]
fit <- lm(Fat~dd,data= tecator)
typeof(dd)
ddd <- as.data.frame(dd)
fit <- lm(Fat~ddd,data= tecator)
fit <- lm(Fat~ dd,data= tecator)
dd <- tecator[,2:102]
fit <- lm(Fat~.,data= tecator)
step <- stepAIC(fit, direction="both")
fit <- lm(Fat~.,data= dd)
?lm
tecator <- read_xlsx("tecator.xlsx")
dd <- tecator[,2:102]
fit <- lm(Fat~.,data= dd)
data = read_excel("tecator.xlsx")
plot(data$Protein,data$Moisture)
set.seed(12345)
n = nrow(data)
train_indexes = sample(1:n,floor(n*0.5))
train_data = data[train_indexes,]
test_data = data[-train_indexes,]
power = 6
train_error = matrix(0,power,1)
test_error = matrix(0,power,1)
for(i in 1:power)
{
model = lm(Moisture ~ poly(Protein,i), data=train_data)
train_predictions = predict(model,train_data)
test_predictions = predict(model,test_data)
train_error[i,] = mean((train_data$Moisture - train_predictions)^2) # MSE Formula below
test_error[i,] = mean((test_data$Moisture - test_predictions)^2)#mean((predicted-actual Data)^2)
}
ylim = c(min(rbind(train_error,test_error)),max(rbind(train_error,test_error)))#Limits of PLOT
plot(1:power,train_error, col="Green", ylim=ylim)
lines(1:power,train_error, col="Green")
points(1:power,test_error,col="Red")
model = lm(Fat ~ ., data=data)
steps = stepAIC(model,direction="both", trace=FALSE)
lines(1:power,test_error, col="Red")
steps = stepAIC(model,direction="both", trace=FALSE)
library(MASS)
steps = stepAIC(model,direction="both", trace=FALSE)
coeff_aics = steps$coefficients
coeff_aics
summary(steps)
coeff_aics = steps$coefficients
n_coeff_aics = length(coeff_aics)
n_coeff_aics
step$anova
step
step$anova
step$anova
steps$anova
?stepAIC
lasso
model = lm(Fat ~ ., data=data)
steps = stepAIC(model,direction="both", trace=FALSE)
coeff_aics = steps$coefficients
n_coeff_aics = length(coeff_aics)
print(n_coeff_aics)
rl_data <- data.matrix(data)
y <- rl_data[,102] # Select Fat as the only response...
X <- rl_data[,2:101] # Select Channel1-Channel-100 features.
ridge <- glmnet(X, y, alpha = 0) # Fit with the Ridge regression. Alpha=0
plot(ridge, xvar="lambda", label=TRUE)
lasso <- glmnet(X, y, alpha = 1) # Fit with the Lasso regression. Alpha=1
library(glmnet)
install.packages("glmnet")
library("glmnet", lib.loc="~/R/win-library/3.5")
rl_data <- data.matrix(data)
y <- rl_data[,102] # Select Fat as the only response...
X <- rl_data[,2:101] # Select Channel1-Channel-100 features.
ridge <- glmnet(X, y, alpha = 0) # Fit with the Ridge regression. Alpha=0
plot(ridge, xvar="lambda", label=TRUE)
lasso <- glmnet(X, y, alpha = 1) # Fit with the Lasso regression. Alpha=1
lasso
ll <- c(lasso,0)
ll
kfoldcv <- cv.glmnet(X, y,  type.measure="mse",lambda = ll, nfolds=20) #cross validation on lasso
kfoldcv <- cv.glmnet(X, y,  type.measure="mse",lambda = ll) #cross validation on lasso
?cv.glmnet
kfoldcv <- cv.glmnet(X, y,lambda = ll,type.measure="mse", nfolds=20) #cross validation on lasso
ll <- c(lasso$lambda,0)
kfoldcv <- cv.glmnet(X, y,lambda = ll,type.measure="mse", nfolds=20) #cross validation on lasso
ll
